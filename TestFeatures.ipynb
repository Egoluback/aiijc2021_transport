{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TestFeatures.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Dn9Yyc2cJ_EH",
        "uebX5L2peiRI"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3_4FxfJIzHp"
      },
      "source": [
        "### **Импорт библиотек**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DDIGZI6IRLv",
        "outputId": "3931fcd7-07d5-4102-c86f-9f0d5de04c70"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6g1759rCIxzC",
        "outputId": "087e5154-8bb4-4bfe-bd77-6b07b9a6992c"
      },
      "source": [
        "!pip install catboost\n",
        "!pip install tqdm\n",
        "!pip install pymorphy2[fast]\n",
        "!pip install fasttext"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-0.26.1-cp37-none-manylinux1_x86_64.whl (67.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 67.4 MB 26 kB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from catboost) (1.15.0)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (from catboost) (4.4.1)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.1.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from catboost) (3.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from catboost) (1.4.1)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from catboost) (0.10.1)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.19.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->catboost) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->catboost) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (0.10.0)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.7/dist-packages (from plotly->catboost) (1.3.3)\n",
            "Installing collected packages: catboost\n",
            "Successfully installed catboost-0.26.1\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.62.0)\n",
            "Collecting pymorphy2[fast]\n",
            "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 2.8 MB/s \n",
            "\u001b[?25hCollecting pymorphy2-dicts-ru<3.0,>=2.4\n",
            "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.2 MB 16.1 MB/s \n",
            "\u001b[?25hCollecting dawg-python>=0.7.1\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.7/dist-packages (from pymorphy2[fast]) (0.6.2)\n",
            "Collecting DAWG>=0.8\n",
            "  Downloading DAWG-0.8.0.tar.gz (371 kB)\n",
            "\u001b[K     |████████████████████████████████| 371 kB 37.1 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: DAWG\n",
            "  Building wheel for DAWG (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for DAWG: filename=DAWG-0.8.0-cp37-cp37m-linux_x86_64.whl size=860122 sha256=8c8efb6cc90b92525dc74ac01a126cec6da5586a08ff181c8f516b490fcc8f9c\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/51/a4/2de41ff197786537075027c27b479a38da92f50abc86634445\n",
            "Successfully built DAWG\n",
            "Installing collected packages: pymorphy2-dicts-ru, dawg-python, pymorphy2, DAWG\n",
            "Successfully installed DAWG-0.8.0 dawg-python-0.7.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844\n",
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[K     |████████████████████████████████| 68 kB 2.1 MB/s \n",
            "\u001b[?25hCollecting pybind11>=2.2\n",
            "  Using cached pybind11-2.7.1-py2.py3-none-any.whl (200 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext) (57.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fasttext) (1.19.5)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp37-cp37m-linux_x86_64.whl size=3091910 sha256=63733d5d95752c6f61eb480d32d09cbb913df0f0e2754622657ff97afb075a51\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/ca/bf/b020d2be95f7641801a6597a29c8f4f19e38f9c02a345bab9b\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.2 pybind11-2.7.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoDyTCGrJOmT",
        "outputId": "b2124506-3137-4235-e53d-5618f01eab36"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import catboost as ctb\n",
        "from sklearn.linear_model import RidgeClassifierCV\n",
        "from sklearn.linear_model import RidgeClassifier\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering, AffinityPropagation, SpectralClustering\n",
        "from sklearn import utils\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_validate, cross_val_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from pymorphy2 import MorphAnalyzer\n",
        "from scipy.spatial.distance import cosine, cityblock, jaccard, canberra, euclidean, minkowski, braycurtis\n",
        "\n",
        "import gensim\n",
        "import gensim.downloader as api\n",
        "from gensim.test.utils import get_tmpfile\n",
        "import fasttext\n",
        "from gensim.models import FastText, KeyedVectors\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dn9Yyc2cJ_EH"
      },
      "source": [
        "### **Считываем и смотрим на данные**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PO3wswvEKKtB"
      },
      "source": [
        "labled_train_data = pd.read_csv('/content/drive/MyDrive/labled_train_data.csv', comment='#', sep='\\t').drop('Unnamed: 0', axis=1)\n",
        "labled_train_comments = pd.read_csv('/content/drive/MyDrive/labled_train_comments.csv', comment='#', sep='\\t').drop('Unnamed: 0', axis=1)\n",
        "labled_train_speed = pd.read_csv('/content/drive/MyDrive/labled_train_tracks_speed.csv', comment='#', sep=',')\n",
        "labled_train_tracks = pd.read_csv('/content/drive/MyDrive/labled_train_tracks.csv', comment='#', sep='\\t').drop('Unnamed: 0', axis=1)\n",
        "\n",
        "unlabled_train_data = pd.read_csv('/content/drive/MyDrive/unlabled_train_data.csv', comment='#', sep='\\t').drop('Unnamed: 0', axis=1)\n",
        "unlabled_train_comments = pd.read_csv('/content/drive/MyDrive/unlabled_train_comments.csv', comment='#', sep='\\t').drop('Unnamed: 0', axis=1)\n",
        "unlabled_train_speed = pd.read_csv('/content/drive/MyDrive/unlabled_train_tracks_speed.csv', comment='#', sep=',')\n",
        "unlabled_train_tracks = pd.read_csv('/content/drive/MyDrive/unlabled_train_tracks.csv', comment='#', sep='\\t').drop('Unnamed: 0', axis=1)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbqUT64L9IlB"
      },
      "source": [
        "labled_train_comments = pd.concat([labled_train_comments, unlabled_train_comments], ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "CV7cM1Eju2D_",
        "outputId": "2ea978aa-71db-4e08-e349-2028a2b5cea7"
      },
      "source": [
        "labled_train_data.head()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>order_id</th>\n",
              "      <th>driver_id</th>\n",
              "      <th>client_id</th>\n",
              "      <th>dttm</th>\n",
              "      <th>date</th>\n",
              "      <th>arrived_distance</th>\n",
              "      <th>arrived_duration</th>\n",
              "      <th>distance</th>\n",
              "      <th>duration</th>\n",
              "      <th>from_latitude</th>\n",
              "      <th>from_longitude</th>\n",
              "      <th>to_latitude</th>\n",
              "      <th>to_longitude</th>\n",
              "      <th>mark</th>\n",
              "      <th>client_rate_ride</th>\n",
              "      <th>client_rides_cnt</th>\n",
              "      <th>driver_rides_cnt</th>\n",
              "      <th>comment</th>\n",
              "      <th>is_aggressive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6a0f322ade1a05e5c4cec4344efbce8b</td>\n",
              "      <td>f7c2b293ef94420f5e51abae6889b83b</td>\n",
              "      <td>3156d05c6458a8228bed59f02075a61e</td>\n",
              "      <td>2021-01-22 21:53:00</td>\n",
              "      <td>2021-01-22</td>\n",
              "      <td>150.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>11.8</td>\n",
              "      <td>20.5</td>\n",
              "      <td>55.795900</td>\n",
              "      <td>37.560300</td>\n",
              "      <td>55.716502</td>\n",
              "      <td>37.524627</td>\n",
              "      <td>Kia K5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>16.0</td>\n",
              "      <td>39.0</td>\n",
              "      <td>Больше нечего сказать</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>934ecbe5845426fd3f8ef7938cce2a11</td>\n",
              "      <td>01d029c42c99581080a60679fca06ff9</td>\n",
              "      <td>3156d05c6458a8228bed59f02075a61e</td>\n",
              "      <td>2021-01-24 14:09:00</td>\n",
              "      <td>2021-01-24</td>\n",
              "      <td>570.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>17.2</td>\n",
              "      <td>26.2</td>\n",
              "      <td>55.716502</td>\n",
              "      <td>37.524627</td>\n",
              "      <td>55.808253</td>\n",
              "      <td>37.638847</td>\n",
              "      <td>Volkswagen Polo</td>\n",
              "      <td>NaN</td>\n",
              "      <td>18.0</td>\n",
              "      <td>338.0</td>\n",
              "      <td>Да</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5348cb339b63eaea3b2cb57a064ce550</td>\n",
              "      <td>3c88deb7df7a73a24ebc229db9783405</td>\n",
              "      <td>3156d05c6458a8228bed59f02075a61e</td>\n",
              "      <td>2021-01-26 21:02:00</td>\n",
              "      <td>2021-01-26</td>\n",
              "      <td>40.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.8</td>\n",
              "      <td>10.9</td>\n",
              "      <td>55.716637</td>\n",
              "      <td>37.524223</td>\n",
              "      <td>55.741958</td>\n",
              "      <td>37.568172</td>\n",
              "      <td>MercedesBenz EClass</td>\n",
              "      <td>NaN</td>\n",
              "      <td>17.0</td>\n",
              "      <td>162.0</td>\n",
              "      <td>Больше нечего сказать</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>309ef91c3b51e27d097642169576f67b</td>\n",
              "      <td>f35a8ff85f2095755f16bba91035fbdc</td>\n",
              "      <td>3156d05c6458a8228bed59f02075a61e</td>\n",
              "      <td>2021-01-27 17:24:00</td>\n",
              "      <td>2021-01-27</td>\n",
              "      <td>140.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.5</td>\n",
              "      <td>10.7</td>\n",
              "      <td>55.689076</td>\n",
              "      <td>37.491089</td>\n",
              "      <td>55.716502</td>\n",
              "      <td>37.524627</td>\n",
              "      <td>Kia Optima</td>\n",
              "      <td>NaN</td>\n",
              "      <td>18.0</td>\n",
              "      <td>136.0</td>\n",
              "      <td>Больше нечего сказать</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3506e04e45d39c6e3033637389da1041</td>\n",
              "      <td>0a227ac8d702170c03acf36d55e60d0d</td>\n",
              "      <td>3156d05c6458a8228bed59f02075a61e</td>\n",
              "      <td>2021-01-29 15:31:00</td>\n",
              "      <td>2021-01-29</td>\n",
              "      <td>150.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>15.4</td>\n",
              "      <td>25.1</td>\n",
              "      <td>55.655489</td>\n",
              "      <td>37.616629</td>\n",
              "      <td>55.716502</td>\n",
              "      <td>37.524627</td>\n",
              "      <td>Kia Rio</td>\n",
              "      <td>5.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>194.0</td>\n",
              "      <td>Больше нечего сказать</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                           order_id  ... is_aggressive\n",
              "0  6a0f322ade1a05e5c4cec4344efbce8b  ...             0\n",
              "1  934ecbe5845426fd3f8ef7938cce2a11  ...             0\n",
              "2  5348cb339b63eaea3b2cb57a064ce550  ...             0\n",
              "3  309ef91c3b51e27d097642169576f67b  ...             0\n",
              "4  3506e04e45d39c6e3033637389da1041  ...             0\n",
              "\n",
              "[5 rows x 19 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uebX5L2peiRI"
      },
      "source": [
        "### **Тест**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XX-JN05delAF"
      },
      "source": [
        "class Model_test:\n",
        "  def __init__(self):\n",
        "    self.model = None\n",
        "    self.clustering_model = KMeans(n_clusters=3, random_state=42, n_init=50, max_iter=300)\n",
        "    self.spectral_clusterer = SpectralClustering(assign_labels='discretize', n_clusters=3,\n",
        "    random_state=42)\n",
        "    self.cars_vectorizer = Doc2Vec(min_count=1, size=8, window=2, workers=-1, seed=42, negative=0)\n",
        "    self.comm_model = LogisticRegression(random_state=42, max_iter=1000)\n",
        "    self.cars_clusterer = KMeans(n_clusters=3, random_state=42, n_init=50, max_iter=300)\n",
        "    self.text_vectorizer = Doc2Vec(min_count=1, size=30, window=5, workers=-1, seed=42, negative=3)\n",
        "    \n",
        "    self.standart_comments = ['Больше нечего сказать'.lower(), 'Да'.lower(), 'Ок'.lower()]\n",
        "    self.aggressive_words = {'verb': set(), 'adj': set(), 'all_words': set(), 'noun': set()}\n",
        "    self.morph_analyzer = MorphAnalyzer()\n",
        "\n",
        "    self.stop_words = set(stopwords.words('russian')) # стоп-слова из nltk\n",
        "    self.stop_words.add('')\n",
        "    self.stop_words.add(' ')\n",
        "    self.stop_words.add('\\t')\n",
        "\n",
        "  # возвращает нормальную форму слова(при normal_form=True, иначе просто слово) и его тэг(характеристики слова)\n",
        "  def word_preprocess(self, word, word_normal_form=False):\n",
        "      word = re.sub(r'[\\d\\W]', '', word).lower().strip() # убирает пробелы, цифры и знаки препинания\n",
        "      word = word.replace('_', '')\n",
        "      w = self.morph_analyzer.parse(word)[0]\n",
        "      if word_normal_form:\n",
        "        return w.normal_form, w.tag\n",
        "      return word, w.tag\n",
        "\n",
        "  def train_doc2vec_model(self, X, y, comm_dataset=None):\n",
        "    print('training vectorizer model...')\n",
        "    dataset = X.join(y).copy()\n",
        "    dataset = dataset.fillna({'comment': self.standart_comments[0]})\n",
        "    # dataset = dataset.loc[dataset['is_aggressive'] == 1]\n",
        "    data = []\n",
        "    tag_n = 0\n",
        "    for row in dataset.itertuples(): # перебираем все строки в датасете\n",
        "        comment = getattr(row, 'comment')\n",
        "        comment = [self.word_preprocess(word, word_normal_form=True)[0] for word in comment.split(\" \") if self.word_preprocess(word, word_normal_form=True)[0] not in self.stop_words]\n",
        "        comment = [word for word in comment if word != '']\n",
        "        if len(comment) > 0 and getattr(row, 'comment').lower().strip() not in self.standart_comments:\n",
        "          data.append(TaggedDocument(comment, [tag_n]))\n",
        "          tag_n += 1\n",
        "    if comm_dataset is not None:\n",
        "      comm_dataset = comm_dataset.fillna({'comment': self.standart_comments[0]})\n",
        "      for row in comm_dataset.itertuples():\n",
        "        comment = getattr(row, 'comment')\n",
        "        comment = [self.word_preprocess(word, word_normal_form=True)[0] for word in comment.split(\" \") if self.word_preprocess(word, word_normal_form=True)[0] not in self.stop_words]\n",
        "        comment = [word for word in comment if word != '']\n",
        "        if len(comment) > 0 and getattr(row, 'comment').lower().strip() not in self.standart_comments:\n",
        "          data.append(TaggedDocument(comment, [tag_n]))\n",
        "          tag_n += 1\n",
        "    self.text_vectorizer.build_vocab(data)\n",
        "    for epoch in range(30):\n",
        "      self.text_vectorizer.train(utils.shuffle(data), total_examples=len(data), epochs=1)\n",
        "      self.text_vectorizer.alpha -= 0.002\n",
        "      self.text_vectorizer.min_alpha = self.text_vectorizer.alpha\n",
        "    print('TEXT WECTORIZER TRAINED')\n",
        "    \n",
        "  # тренировка векторизатора машин\n",
        "  def train_cars_vectorizer_and_clusterer(self, X, y):\n",
        "      print('training vectorizer model...')\n",
        "      dataset = X.join(y).copy()\n",
        "      data = []\n",
        "      tag_n = 0\n",
        "      for row in dataset.itertuples(): # перебираем все строки в датасете\n",
        "          if getattr(row, 'mark'):\n",
        "            car = getattr(row, 'mark')\n",
        "            car = [word.lower().strip() for word in car.split(\" \")]\n",
        "            car = [word for word in car if word != '']\n",
        "            if len(car) > 0:\n",
        "              data.append(TaggedDocument(car, [tag_n]))\n",
        "              tag_n += 1\n",
        "      self.cars_vectorizer.build_vocab(data)\n",
        "      for epoch in range(20):\n",
        "        self.cars_vectorizer.train(utils.shuffle(data), total_examples=len(data), epochs=1)\n",
        "        self.cars_vectorizer.alpha -= 0.002\n",
        "        self.cars_vectorizer.min_alpha = self.cars_vectorizer.alpha\n",
        "      print('CARS WECTORIZER TRAINED')\n",
        "      print('training cars clustering model...')\n",
        "      vectors = []\n",
        "      c = 0\n",
        "      for row in dataset.itertuples(): # перебираем все строки в датасете\n",
        "        if getattr(row, 'mark'):\n",
        "          car = getattr(row, 'mark')\n",
        "          car = [word.lower().strip() for word in car.split(\" \")]\n",
        "          car = [word for word in car if word != '']\n",
        "          if len(car) > 0:\n",
        "            vectors.append(self.cars_vectorizer.infer_vector(car))\n",
        "      self.cars_clusterer.fit(vectors)\n",
        "      print('CARS CLUSTERING COMPLETED')\n",
        "\n",
        "  # генерация фич из кластеров\n",
        "  def cluster_features(self, text):\n",
        "    similarity = []\n",
        "    clusters = self.clustering_model.cluster_centers_\n",
        "    prep_text = [self.word_preprocess(word, word_normal_form=True)[0] for word in text.split(\" \") if self.word_preprocess(word, word_normal_form=True)[0] not in self.stop_words]\n",
        "    prep_text = [word for word in text if word != '']\n",
        "    if len(prep_text) == 0 or text.lower().strip() in self.standart_comments:\n",
        "      for i in range(len(clusters)):\n",
        "        similarity.append([0, len(clusters)])\n",
        "      return similarity\n",
        "\n",
        "    vector = self.text_vectorizer.infer_vector(prep_text)\n",
        "    for i in range(len(clusters)):\n",
        "      similarity.append([cosine(vector, clusters[i]), self.clustering_model.predict([vector])[0]])\n",
        "    return similarity\n",
        "\n",
        "  def train_comm_model(self, X, y):\n",
        "    dataset = X.join(y).copy()\n",
        "    dataset = dataset.fillna({'comment': self.standart_comments[0]})\n",
        "    vectors = []\n",
        "    c = 0\n",
        "    train_y = []\n",
        "    for row in dataset.itertuples(): # перебираем все строки в датасете\n",
        "      comment = getattr(row, 'comment')\n",
        "      comment = [self.word_preprocess(word, word_normal_form=True)[0] for word in comment.split(\" \") if self.word_preprocess(word, word_normal_form=True)[0] not in self.stop_words]\n",
        "      comment = [word for word in comment if word != '']\n",
        "      if len(comment) > 0 and getattr(row, 'comment').lower().strip() not in self.standart_comments:\n",
        "        vectors.append(self.text_vectorizer.infer_vector(comment))\n",
        "        train_y.append(getattr(row, 'is_aggressive'))\n",
        "    self.clustering_model.fit(vectors)\n",
        "    print('CLUSTERING COMPLETED')\n",
        "    self.comm_model.fit(vectors, train_y)\n",
        "\n",
        "  # средний рейтинг по комментариям для каждого водителя  (плохо работает, хз че с ними делать), не юзать пока\n",
        "  def mean_comments_aggressive_rate(self, comm_dataset, X):\n",
        "    mean_comments_aggressive_rate = []\n",
        "    comm_dataset = comm_dataset.fillna({'comment': self.standart_comments[0]})\n",
        "    for drv_id in X['driver_id']:\n",
        "      driver_comments_rate = []\n",
        "      driver_comments = comm_dataset.loc[comm_dataset['driver_id'] == drv_id]\n",
        "      for row in driver_comments.itertuples():\n",
        "        commentt = getattr(row, 'comment')\n",
        "        commentt = [self.word_preprocess(word, word_normal_form=True)[0] for word in commentt.split(\" \") if self.word_preprocess(word, word_normal_form=True)[0] not in self.stop_words]\n",
        "        commentt = [word for word in commentt if word != '']\n",
        "        if getattr(row, 'comment').lower().strip() not in self.standart_comments and len(commentt) > 0:\n",
        "          driver_comments_rate.append(self.comm_model.predict_proba([self.text_vectorizer.infer_vector(commentt)])[:, 1][0])\n",
        "        else:\n",
        "          driver_comments_rate.append(0)\n",
        "      if len(driver_comments_rate) == 0:\n",
        "        driver_comments_rate.append(0)\n",
        "      mean_comments_aggressive_rate.append(np.mean(driver_comments_rate))\n",
        "    return mean_comments_aggressive_rate\n",
        "\n",
        "  # заполнение словаря агрессивными словами\n",
        "  def fill_agressive_vocab(self, X, y):\n",
        "    dataset = X.join(y).copy()\n",
        "    for row in dataset.itertuples(): # перебираем все строки в датасете\n",
        "      if getattr(row, 'is_aggressive') == 1 and getattr(row, 'comment') and getattr(row, 'comment').lower().strip() not in self.standart_comments:\n",
        "        words = [self.word_preprocess(word, word_normal_form=True) for word in getattr(row, 'comment').split(' ')]\n",
        "        for word in words:\n",
        "          if word[0] not in self.stop_words:\n",
        "            if 'VERB' in word[1]:\n",
        "              self.aggressive_words['verb'].add(word[0])\n",
        "            elif 'ADJF' in word[1] or 'ADJS' in word[1]:\n",
        "              self.aggressive_words['adj'].add(word[0])\n",
        "            elif 'NOUN' in word[1]:\n",
        "              self.aggressive_words['noun'].add(word[0])\n",
        "\n",
        "  # делаем NLP фичи на основе сгенерированного словаря\n",
        "  def NLP_feature_extract(self, X, y=None):\n",
        "    agg_verbs_rate = [] # глаголы\n",
        "    agg_adjs_rate = [] # прилагательные\n",
        "    agg_nouns_rate = [] # существительные\n",
        "    for row in X.itertuples(): # перебираем все строки в датасете\n",
        "      if getattr(row, 'comment'):\n",
        "        words = [self.word_preprocess(word, word_normal_form=True) for word in getattr(row, 'comment').split(' ')]\n",
        "        words_verb = [word[0] for word in words if 'VERB' in word[1] and word[0] not in self.stop_words]\n",
        "        words_adj = [word[0] for word in words if ('ADJF' in word[1] or 'ADJS' in word[1]) and word[0] not in self.stop_words]\n",
        "        words_noun = [word[0] for word in words if 'NOUN' in word[1] and word[0] not in self.stop_words]\n",
        "\n",
        "        if len(words_verb) > 0 and getattr(row, 'comment').lower().strip() not in self.standart_comments:\n",
        "          agg_verbs_rate.append(len(set(words_verb) & self.aggressive_words['verb']) / len(words_verb))\n",
        "        else:\n",
        "          agg_verbs_rate.append(0)\n",
        "\n",
        "        if len(words_adj) > 0 and getattr(row, 'comment').lower().strip() not in self.standart_comments:\n",
        "          agg_adjs_rate.append(len(set(words_adj) & self.aggressive_words['adj']) / len(words_adj))\n",
        "        else:\n",
        "          agg_adjs_rate.append(0)\n",
        "\n",
        "        if len(words_noun) > 0 and getattr(row, 'comment').lower().strip() not in self.standart_comments:\n",
        "          agg_nouns_rate.append(len(set(words_noun) & self.aggressive_words['noun']) / len(words_noun))\n",
        "        else:\n",
        "          agg_nouns_rate.append(0)\n",
        "\n",
        "    return agg_verbs_rate, agg_adjs_rate, agg_nouns_rate\n",
        "\n",
        "  # отбор фичей\n",
        "  def features(self, X, comm_dataset=None):\n",
        "    data = X.copy()\n",
        "    agg_verbs_rate, agg_adjs_rate, agg_nouns_rate = self.NLP_feature_extract(data)\n",
        "\n",
        "    data['agg_verbs_rate'] = agg_verbs_rate\n",
        "    data['agg_adjs_rate'] = agg_adjs_rate\n",
        "    data['agg_nouns_rate'] = agg_nouns_rate\n",
        "\n",
        "    feature_list = ['agg_verbs_rate', 'agg_adjs_rate', 'agg_nouns_rate']\n",
        "\n",
        "    if comm_dataset is not None:\n",
        "      data['mean_comments_aggressive_rate'] = self.mean_comments_aggressive_rate(comm_dataset, X)\n",
        "      feature_list.append('mean_comments_aggressive_rate')\n",
        "    \n",
        "    probabilities = []\n",
        "    for row in data.itertuples(): # перебираем все строки в датасете\n",
        "       comment = getattr(row, 'comment')\n",
        "       comment = [self.word_preprocess(word, word_normal_form=True)[0] for word in comment.split(\" \") if self.word_preprocess(word, word_normal_form=True)[0] not in self.stop_words]\n",
        "       comment = [word for word in comment if word != '']\n",
        "       if len(comment) > 0 and getattr(row, 'comment').lower().strip() not in self.standart_comments:\n",
        "         probabilities.append(self.comm_model.predict_proba([self.text_vectorizer.infer_vector(comment)])[:, 1][0])\n",
        "       else: \n",
        "         probabilities.append(0)\n",
        "\n",
        "    data['agg_comm_probability'] = probabilities\n",
        "    feature_list.append('agg_comm_probability')\n",
        "\n",
        "    cars_cluster = []\n",
        "    for row in data.itertuples(): # перебираем все строки в датасете\n",
        "       car = getattr(row, 'mark')\n",
        "       car = [word.lower().strip() for word in car.split(\" \")]\n",
        "       car = [word for word in car if word != '']\n",
        "       cars_cluster.append(self.cars_clusterer.predict([self.cars_vectorizer.infer_vector(car)])[0])\n",
        "    data['cars_cluster'] = cars_cluster\n",
        "    feature_list.append('cars_cluster')\n",
        "\n",
        "    # vectors = []\n",
        "    # for row in data.itertuples(): # перебираем все строки в датасете\n",
        "    #   comment = getattr(row, 'comment')\n",
        "    #   comment = [self.word_preprocess(word, word_normal_form=True)[0] for word in comment.split(\" \") if self.word_preprocess(word, word_normal_form=True)[0] not in self.stop_words]\n",
        "    #   comment = [word for word in comment if word != '']\n",
        "    #   if len(comment) == 0:\n",
        "    #     comment = [self.word_preprocess(word, word_normal_form=True)[0] for word in self.standart_comments[0].split(\" \") if self.word_preprocess(word, word_normal_form=True)[0] not in self.stop_words]\n",
        "    #   vectors.append(self.text_vectorizer.infer_vector(comment))\n",
        "    # data['spectral_clusters'] = self.spectral_clustering(vectors)\n",
        "    # feature_list.append('spectral_clusters')\n",
        "\n",
        "    # similarities = []\n",
        "    # for row in data.itertuples(): # перебираем все строки в датасете\n",
        "    #   comment = getattr(row, 'comment')\n",
        "    #   similarities.append(self.cluster_features(comment))\n",
        "\n",
        "\n",
        "    # for cluster in range(len(self.clustering_model.cluster_centers_)):\n",
        "    #   data[f\"euclidean_cluster_{cluster}\"] = [similarities[i][cluster][0] for i in range(len(similarities))]\n",
        "    #   feature_list.append(f\"euclidean_cluster_{cluster}\")\n",
        "    # data['cluster'] = [similarities[i][0][1] for i in range(len(similarities))]\n",
        "    # feature_list.append('cluster')\n",
        "\n",
        "    data['is_comment'] = [1 if getattr(row, 'comment') and getattr(row, 'comment').lower().strip() not in self.standart_comments else 0 for row in data.itertuples()]\n",
        "    feature_list.append('is_comment')\n",
        "\n",
        "    # заполним NaN средними значениями\n",
        "    for feature in feature_list:\n",
        "      data = data.fillna({feature: data[feature].mean()})\n",
        "    \n",
        "    data = data.set_index('order_id')\n",
        "    return data[feature_list]\n",
        "\n",
        "  # кросс-валидация и предикт на тесте\n",
        "  def train_eval(self, X, y, comm_dataset_labled=None, comm_dataset_unlabled=None):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "    X_train = X_train.fillna({'comment': self.standart_comments[0]})\n",
        "    X_test = X_test.fillna({'comment': self.standart_comments[0]})\n",
        "\n",
        "    self.fill_agressive_vocab(X_train, y_train) # заполнение словаря агрессивных слов\n",
        "    self.train_doc2vec_model(X_train, y_train, comm_dataset_labled) # тренировка doc2vec модели и кластеризации для комментариев о поездке\n",
        "    self.train_cars_vectorizer_and_clusterer(X_train, y_train) # тренировка doc2vec модели и кластеризации для машин\n",
        "    self.train_comm_model(X_train, y_train) # тренировка модели вероятностей агрессивности текстов\n",
        "\n",
        "    X_train_features = self.features(X_train, comm_dataset_labled)\n",
        "\n",
        "    self.model = LogisticRegression(random_state=42, max_iter=1000)\n",
        "\n",
        "    cv_score = cross_val_score(self.model, X_train_features, y_train, cv=5, scoring='roc_auc')\n",
        "    \n",
        "    self.model.fit(X_train_features, y_train)\n",
        "\n",
        "    print('Test Roc-Auc score:', roc_auc_score(y_test, self.model.predict_proba(self.features(X_test, comm_dataset_labled))[:, 1]))\n",
        "    print('Train Roc-Auc score:', roc_auc_score(y_train, self.model.predict_proba(X_train_features)[:, 1]))\n",
        "    print(f\"CV_mean roc_auc: {np.mean(cv_score)}, CV_folds_score: {cv_score}\")\n",
        "    return X_train_features\n",
        "  \n",
        "  def predict(self, X, comm_dataset=None):\n",
        "    res = self.model.predict_proba(self.features(X, comm_dataset))[:, 1]\n",
        "    X = X.fillna({'comment': self.standart_comments[0]})\n",
        "    datasss = pd.DataFrame()\n",
        "    datasss['is_aggressive'] = res\n",
        "    datasss.to_csv('resss.csv')\n",
        "    print('prediction saved')"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKhaUcEMpiCt",
        "outputId": "8dfbf493-dadf-4b63-9ba1-0a46ef28497f"
      },
      "source": [
        "\n",
        "model_test = Model_test()\n",
        "features = model_test.train_eval(X, y, comm_dataset_labled=None)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training vectorizer model...\n",
            "TEXT WECTORIZER TRAINED\n",
            "training vectorizer model...\n",
            "CARS WECTORIZER TRAINED\n",
            "training cars clustering model...\n",
            "CARS CLUSTERING COMPLETED\n",
            "CLUSTERING COMPLETED\n",
            "Test Roc-Auc score: 0.6599572615941547\n",
            "Train Roc-Auc score: 0.7110839103825374\n",
            "CV_mean roc_auc: 0.7095085388788034, CV_folds_score: [0.70010653 0.71683099 0.70772684 0.68695851 0.73591982]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyLlzoTz9ZfW",
        "outputId": "77bfbc3c-e9a0-4787-9c6e-e59d5e065e69"
      },
      "source": [
        "model_test.predict(labled_test_data, comm_dataset=labled_train_comments)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "prediction saved\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "bNBLwJmN5VMo",
        "outputId": "6163dd83-e747-419e-8022-cd7f19cdf6b8"
      },
      "source": [
        "features.head(100)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>agg_verbs_rate</th>\n",
              "      <th>agg_adjs_rate</th>\n",
              "      <th>agg_nouns_rate</th>\n",
              "      <th>agg_comm_probability</th>\n",
              "      <th>cars_cluster</th>\n",
              "      <th>is_comment</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>order_id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>abc3159a76397972ed5bc025093ee6ee</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8a79ddc85d97f0936e3de39622a8647e</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1fb57420a9924e9bbae9768d113283c3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4d338112139fea37dcc4fa9c3d7ac14f</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.162616</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17968f160d85183f977a126297552deb</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22e5f7df64bf76a0a3f521417871f169</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.162957</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8bb9be586bf1eeccffdfcdbe265fcbd3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>830f2076f815ba5f9944c33880cfb4fa</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>f680716c84af81fbe2127e2044b0cd28</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20b3f302d42f879094ebece394f70ef9</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                  agg_verbs_rate  ...  is_comment\n",
              "order_id                                          ...            \n",
              "abc3159a76397972ed5bc025093ee6ee             0.0  ...           0\n",
              "8a79ddc85d97f0936e3de39622a8647e             0.0  ...           0\n",
              "1fb57420a9924e9bbae9768d113283c3             0.0  ...           0\n",
              "4d338112139fea37dcc4fa9c3d7ac14f             1.0  ...           1\n",
              "17968f160d85183f977a126297552deb             0.0  ...           0\n",
              "...                                          ...  ...         ...\n",
              "22e5f7df64bf76a0a3f521417871f169             0.0  ...           1\n",
              "8bb9be586bf1eeccffdfcdbe265fcbd3             0.0  ...           0\n",
              "830f2076f815ba5f9944c33880cfb4fa             0.0  ...           0\n",
              "f680716c84af81fbe2127e2044b0cd28             0.0  ...           0\n",
              "20b3f302d42f879094ebece394f70ef9             0.0  ...           0\n",
              "\n",
              "[100 rows x 6 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujZ8p_kuW4Co"
      },
      "source": [
        "### **Модель**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbdiJIpH0oFj"
      },
      "source": [
        "labled_train_data = pd.read_csv('/content/drive/MyDrive/labled_train_data.csv', comment='#', sep='\\t').drop('Unnamed: 0', axis=1)\n",
        "labled_train_comments = pd.read_csv('/content/drive/MyDrive/labled_train_comments.csv', comment='#', sep='\\t').drop('Unnamed: 0', axis=1)\n",
        "labled_train_speed = pd.read_csv('/content/drive/MyDrive/labled_train_tracks_speed.csv', comment='#', sep=',')\n",
        "labled_train_tracks = pd.read_csv('/content/drive/MyDrive/labled_train_tracks.csv', comment='#', sep='\\t').drop('Unnamed: 0', axis=1)\n",
        "\n",
        "unlabled_train_data = pd.read_csv('/content/drive/MyDrive/unlabled_train_data.csv', comment='#', sep='\\t').drop('Unnamed: 0', axis=1)\n",
        "unlabled_train_comments = pd.read_csv('/content/drive/MyDrive/unlabled_train_comments.csv', comment='#', sep='\\t').drop('Unnamed: 0', axis=1)\n",
        "unlabled_train_speed = pd.read_csv('/content/drive/MyDrive/unlabled_train_tracks_speed.csv', comment='#', sep=',')\n",
        "unlabled_train_tracks = pd.read_csv('/content/drive/MyDrive/unlabled_train_tracks.csv', comment='#', sep='\\t').drop('Unnamed: 0', axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5k6gy6dYqcdS"
      },
      "source": [
        "X, y = labled_train_data[labled_train_data.columns[:-1]], labled_train_data['is_aggressive']"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdXibIW4YBwr"
      },
      "source": [
        "labled_test_data = pd.read_csv('/content/drive/MyDrive/labled_test_data.csv', comment='#', sep='\\t').drop('Unnamed: 0', axis=1)"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4wZyoWUU_IA"
      },
      "source": [
        "class Model:\n",
        "  def __init__(self):\n",
        "    self.model = None\n",
        "    self.cars_vectorizer = Doc2Vec(min_count=1, size=8, window=2, workers=-1, seed=42)\n",
        "    self.comm_model = LogisticRegression(random_state=42, max_iter=1000)\n",
        "    self.cars_clusterer = KMeans(n_clusters=4, random_state=42, n_init=50, max_iter=300)\n",
        "    self.text_vectorizer = Doc2Vec(min_count=1, size=30, window=5, workers=-1, seed=42)\n",
        "    \n",
        "    self.standart_comments = ['Больше нечего сказать'.lower(), 'Да'.lower(), 'Ок'.lower()]\n",
        "    self.aggressive_words = {'verb': set(), 'adj': set(), 'all_words': set(), 'noun': set()}\n",
        "    self.morph_analyzer = MorphAnalyzer()\n",
        "\n",
        "    self.stop_words = set(stopwords.words('russian')) # стоп-слова из nltk\n",
        "    self.stop_words.add('')\n",
        "    self.stop_words.add(' ')\n",
        "    self.stop_words.add('\\t')\n",
        "\n",
        "  # возвращает нормальную форму слова(при normal_form=True, иначе просто слово) и его тэг(характеристики слова)\n",
        "  def word_preprocess(self, word, word_normal_form=False):\n",
        "      word = re.sub(r'[\\d\\W]', '', word).lower().strip() # убирает пробелы, цифры и знаки препинания\n",
        "      word = word.replace('_', '')\n",
        "      w = self.morph_analyzer.parse(word)[0]\n",
        "      if word_normal_form:\n",
        "        return w.normal_form, w.tag\n",
        "      return word, w.tag\n",
        "\n",
        "  def train_doc2vec_model(self, X, y, comm_dataset=None):\n",
        "    print('training vectorizer model...')\n",
        "    dataset = X.join(y).copy()\n",
        "    dataset = dataset.fillna({'comment': self.standart_comments[0]})\n",
        "    data = []\n",
        "    tag_n = 0\n",
        "    for row in dataset.itertuples(): # перебираем все строки в датасете\n",
        "        comment = getattr(row, 'comment')\n",
        "        comment = [self.word_preprocess(word, word_normal_form=True)[0] for word in comment.split(\" \") if self.word_preprocess(word, word_normal_form=True)[0] not in self.stop_words]\n",
        "        comment = [word for word in comment if word != '']\n",
        "        if len(comment) > 0 and getattr(row, 'comment').lower().strip() not in self.standart_comments:\n",
        "          data.append(TaggedDocument(comment, [tag_n]))\n",
        "          tag_n += 1\n",
        "    if comm_dataset is not None:\n",
        "      comm_dataset = comm_dataset.fillna({'comment': self.standart_comments[0]})\n",
        "      for row in comm_dataset.itertuples():\n",
        "        comment = getattr(row, 'comment')\n",
        "        comment = [self.word_preprocess(word, word_normal_form=True)[0] for word in comment.split(\" \") if self.word_preprocess(word, word_normal_form=True)[0] not in self.stop_words]\n",
        "        comment = [word for word in comment if word != '']\n",
        "        if len(comment) > 0 and getattr(row, 'comment').lower().strip() not in self.standart_comments:\n",
        "          data.append(TaggedDocument(comment, [tag_n]))\n",
        "          tag_n += 1\n",
        "    self.text_vectorizer.build_vocab(data)\n",
        "    for epoch in range(30):\n",
        "      self.text_vectorizer.train(utils.shuffle(data), total_examples=len(data), epochs=1)\n",
        "      self.text_vectorizer.alpha -= 0.002\n",
        "      self.text_vectorizer.min_alpha = self.text_vectorizer.alpha\n",
        "    print('TEXT WECTORIZER TRAINED')\n",
        "    \n",
        "  # тренировка векторизатора машин\n",
        "  def train_cars_vectorizer_and_clusterer(self, X, y):\n",
        "      print('training vectorizer model...')\n",
        "      dataset = X.join(y).copy()\n",
        "      data = []\n",
        "      tag_n = 0\n",
        "      for row in dataset.itertuples(): # перебираем все строки в датасете\n",
        "          if getattr(row, 'mark'):\n",
        "            car = getattr(row, 'mark')\n",
        "            car = [word.lower().strip() for word in car.split(\" \")]\n",
        "            car = [word for word in car if word != '']\n",
        "            if len(car) > 0:\n",
        "              data.append(TaggedDocument(car, [tag_n]))\n",
        "              tag_n += 1\n",
        "      self.cars_vectorizer.build_vocab(data)\n",
        "      for epoch in range(20):\n",
        "        self.cars_vectorizer.train(utils.shuffle(data), total_examples=len(data), epochs=1)\n",
        "        self.cars_vectorizer.alpha -= 0.002\n",
        "        self.cars_vectorizer.min_alpha = self.cars_vectorizer.alpha\n",
        "      print('CARS WECTORIZER TRAINED')\n",
        "      print('training cars clustering model...')\n",
        "      vectors = []\n",
        "      c = 0\n",
        "      for row in dataset.itertuples(): # перебираем все строки в датасете\n",
        "        if getattr(row, 'mark'):\n",
        "          car = getattr(row, 'mark')\n",
        "          car = [word.lower().strip() for word in car.split(\" \")]\n",
        "          car = [word for word in car if word != '']\n",
        "          if len(car) > 0:\n",
        "            vectors.append(self.cars_vectorizer.infer_vector(car))\n",
        "      self.cars_clusterer.fit(vectors)\n",
        "      print('CARS CLUSTERING COMPLETED')\n",
        "\n",
        "  def train_comm_model(self, X, y):\n",
        "    dataset = X.join(y).copy()\n",
        "    dataset = dataset.fillna({'comment': self.standart_comments[0]})\n",
        "    vectors = []\n",
        "    c = 0\n",
        "    train_y = []\n",
        "    for row in dataset.itertuples(): # перебираем все строки в датасете\n",
        "      comment = getattr(row, 'comment')\n",
        "      comment = [self.word_preprocess(word, word_normal_form=True)[0] for word in comment.split(\" \") if self.word_preprocess(word, word_normal_form=True)[0] not in self.stop_words]\n",
        "      comment = [word for word in comment if word != '']\n",
        "      if len(comment) > 0 and getattr(row, 'comment').lower().strip() not in self.standart_comments:\n",
        "        vectors.append(self.text_vectorizer.infer_vector(comment))\n",
        "        train_y.append(getattr(row, 'is_aggressive'))\n",
        "    self.comm_model.fit(vectors, train_y)\n",
        "    print('COMM MODEL TRAINED')\n",
        "\n",
        "  # средний рейтинг по комментариям для каждого водителя  (плохо работает, хз че с ними делать), не юзать пока\n",
        "  def mean_comments_aggressive_rate(self, comm_dataset, X):\n",
        "    mean_comments_aggressive_rate = []\n",
        "    comm_dataset = comm_dataset.fillna({'comment': self.standart_comments[0]})\n",
        "    for drv_id in X['driver_id']:\n",
        "      driver_comments_rate = []\n",
        "      driver_comments = comm_dataset.loc[comm_dataset['driver_id'] == drv_id]\n",
        "      for row in driver_comments.itertuples():\n",
        "        commentt = getattr(row, 'comment')\n",
        "        commentt = [self.word_preprocess(word, word_normal_form=True)[0] for word in commentt.split(\" \") if self.word_preprocess(word, word_normal_form=True)[0] not in self.stop_words]\n",
        "        commentt = [word for word in commentt if word != '']\n",
        "        if getattr(row, 'comment').lower().strip() not in self.standart_comments and len(commentt) > 0:\n",
        "          driver_comments_rate.append(self.comm_model.predict_proba([self.text_vectorizer.infer_vector(commentt)])[:, 1][0])\n",
        "        else:\n",
        "          driver_comments_rate.append(0)\n",
        "      if len(driver_comments_rate) == 0:\n",
        "        driver_comments_rate.append(0)\n",
        "      mean_comments_aggressive_rate.append(np.mean(driver_comments_rate))\n",
        "    return mean_comments_aggressive_rate\n",
        "\n",
        "  # заполнение словаря агрессивными словами\n",
        "  def fill_agressive_vocab(self, X, y):\n",
        "    dataset = X.join(y).copy()\n",
        "    for row in dataset.itertuples(): # перебираем все строки в датасете\n",
        "      if getattr(row, 'is_aggressive') == 1 and getattr(row, 'comment') and getattr(row, 'comment').lower().strip() not in self.standart_comments:\n",
        "        words = [self.word_preprocess(word, word_normal_form=True) for word in getattr(row, 'comment').split(' ')]\n",
        "        for word in words:\n",
        "          if word[0] not in self.stop_words:\n",
        "            if 'VERB' in word[1]:\n",
        "              self.aggressive_words['verb'].add(word[0])\n",
        "            elif 'ADJF' in word[1] or 'ADJS' in word[1]:\n",
        "              self.aggressive_words['adj'].add(word[0])\n",
        "            elif 'NOUN' in word[1]:\n",
        "              self.aggressive_words['noun'].add(word[0])\n",
        "\n",
        "  # делаем NLP фичи на основе сгенерированного словаря\n",
        "  def NLP_feature_extract(self, X, y=None):\n",
        "    agg_verbs_rate = [] # глаголы\n",
        "    agg_adjs_rate = [] # прилагательные\n",
        "    agg_nouns_rate = [] # существительные\n",
        "    for row in X.itertuples(): # перебираем все строки в датасете\n",
        "      if getattr(row, 'comment'):\n",
        "        words = [self.word_preprocess(word, word_normal_form=True) for word in getattr(row, 'comment').split(' ')]\n",
        "        words_verb = [word[0] for word in words if 'VERB' in word[1] and word[0] not in self.stop_words]\n",
        "        words_adj = [word[0] for word in words if ('ADJF' in word[1] or 'ADJS' in word[1]) and word[0] not in self.stop_words]\n",
        "        words_noun = [word[0] for word in words if 'NOUN' in word[1] and word[0] not in self.stop_words]\n",
        "\n",
        "        if len(words_verb) > 0 and getattr(row, 'comment').lower().strip() not in self.standart_comments:\n",
        "          agg_verbs_rate.append(len(set(words_verb) & self.aggressive_words['verb']) / len(words_verb))\n",
        "        else:\n",
        "          agg_verbs_rate.append(0)\n",
        "\n",
        "        if len(words_adj) > 0 and getattr(row, 'comment').lower().strip() not in self.standart_comments:\n",
        "          agg_adjs_rate.append(len(set(words_adj) & self.aggressive_words['adj']) / len(words_adj))\n",
        "        else:\n",
        "          agg_adjs_rate.append(0)\n",
        "\n",
        "        if len(words_noun) > 0 and getattr(row, 'comment').lower().strip() not in self.standart_comments:\n",
        "          agg_nouns_rate.append(len(set(words_noun) & self.aggressive_words['noun']) / len(words_noun))\n",
        "        else:\n",
        "          agg_nouns_rate.append(0)\n",
        "\n",
        "    return agg_verbs_rate, agg_adjs_rate, agg_nouns_rate\n",
        "\n",
        "  # отбор фичей\n",
        "  def features(self, X, comm_dataset=None):\n",
        "    data = X.copy()\n",
        "    agg_verbs_rate, agg_adjs_rate, agg_nouns_rate = self.NLP_feature_extract(data)\n",
        "\n",
        "    data['agg_verbs_rate'] = agg_verbs_rate\n",
        "    data['agg_adjs_rate'] = agg_adjs_rate\n",
        "    data['agg_nouns_rate'] = agg_nouns_rate\n",
        "\n",
        "    feature_list = ['agg_verbs_rate', 'agg_adjs_rate', 'agg_nouns_rate']\n",
        "\n",
        "    if comm_dataset is not None:\n",
        "      data['mean_comments_aggressive_rate'] = self.mean_comments_aggressive_rate(comm_dataset, X)\n",
        "      feature_list.append('mean_comments_aggressive_rate')\n",
        "    \n",
        "    probabilities = []\n",
        "    for row in data.itertuples(): # перебираем все строки в датасете\n",
        "       comment = getattr(row, 'comment')\n",
        "       comment = [self.word_preprocess(word, word_normal_form=True)[0] for word in comment.split(\" \") if self.word_preprocess(word, word_normal_form=True)[0] not in self.stop_words]\n",
        "       comment = [word for word in comment if word != '']\n",
        "       if len(comment) > 0 and getattr(row, 'comment').lower().strip() not in self.standart_comments:\n",
        "         probabilities.append(self.comm_model.predict_proba([self.text_vectorizer.infer_vector(comment)])[:, 1][0])\n",
        "       else: \n",
        "         probabilities.append(0)\n",
        "\n",
        "    data['agg_comm_probability'] = probabilities\n",
        "    feature_list.append('agg_comm_probability')\n",
        "\n",
        "    cars_cluster = []\n",
        "    for row in data.itertuples(): # перебираем все строки в датасете\n",
        "       car = getattr(row, 'mark')\n",
        "       car = [word.lower().strip() for word in car.split(\" \")]\n",
        "       car = [word for word in car if word != '']\n",
        "       cars_cluster.append(self.cars_clusterer.predict([self.cars_vectorizer.infer_vector(car)])[0])\n",
        "    data['cars_cluster'] = cars_cluster\n",
        "    feature_list.append('cars_cluster')\n",
        "\n",
        "    data['is_comment'] = [1 if getattr(row, 'comment') and getattr(row, 'comment').lower().strip() not in self.standart_comments else 0 for row in data.itertuples()]\n",
        "    feature_list.append('is_comment')\n",
        "\n",
        "    # заполним NaN средними значениями\n",
        "    for feature in feature_list:\n",
        "      data = data.fillna({feature: data[feature].mean()})\n",
        "    \n",
        "    data = data.set_index('order_id')\n",
        "    return data[feature_list]\n",
        "\n",
        "  # кросс-валидация и предикт на тесте\n",
        "  def train_eval(self, X, y, comm_dataset_labled=None, comm_dataset_unlabled=None):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "    X_train = X_train.fillna({'comment': self.standart_comments[0]})\n",
        "    X_test = X_test.fillna({'comment': self.standart_comments[0]})\n",
        "\n",
        "    self.fill_agressive_vocab(X_train, y_train) # заполнение словаря агрессивных слов\n",
        "    self.train_doc2vec_model(X_train, y_train, comm_dataset_labled) # тренировка doc2vec модели и кластеризации для комментариев о поездке\n",
        "    self.train_cars_vectorizer_and_clusterer(X_train, y_train) # тренировка doc2vec модели и кластеризации для машин\n",
        "    self.train_comm_model(X_train, y_train) # тренировка модели вероятностей агрессивности текстов\n",
        "\n",
        "    X_train_features = self.features(X_train, comm_dataset_labled)\n",
        "\n",
        "    self.model = LogisticRegression(random_state=42, max_iter=1000)\n",
        "\n",
        "    cv_score = cross_val_score(self.model, X_train_features, y_train, cv=5, scoring='roc_auc')\n",
        "    \n",
        "    self.model.fit(X_train_features, y_train)\n",
        "\n",
        "    print('Test Roc-Auc score:', roc_auc_score(y_test, self.model.predict_proba(self.features(X_test, comm_dataset_labled))[:, 1]))\n",
        "    print('Train Roc-Auc score:', roc_auc_score(y_train, self.model.predict_proba(X_train_features)[:, 1]))\n",
        "    print(f\"CV_mean roc_auc: {np.mean(cv_score)}, CV_folds_score: {cv_score}\")\n",
        "    return X_train_features\n",
        "  \n",
        "  def predict(self, X, comm_dataset=None):\n",
        "    res = self.model.predict_proba(self.features(X, comm_dataset))[:, 1]\n",
        "    X = X.fillna({'comment': self.standart_comments[0]})\n",
        "    datasss = pd.DataFrame()\n",
        "    datasss['is_aggressive'] = res\n",
        "    datasss.to_csv('resss.csv')\n",
        "    print('prediction saved')"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNOX8l_zqnXX",
        "outputId": "46482e69-9388-4470-df16-669c90e85767"
      },
      "source": [
        "model = Model()\n",
        "features = model.train_eval(X, y, comm_dataset_labled=None)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training vectorizer model...\n",
            "TEXT WECTORIZER TRAINED\n",
            "training vectorizer model...\n",
            "CARS WECTORIZER TRAINED\n",
            "training cars clustering model...\n",
            "CARS CLUSTERING COMPLETED\n",
            "COMM MODEL TRAINED\n",
            "Test Roc-Auc score: 0.6556009300689736\n",
            "Train Roc-Auc score: 0.7032904393750704\n",
            "CV_mean roc_auc: 0.7018549220542715, CV_folds_score: [0.66870914 0.73824342 0.66279588 0.72440212 0.71512406]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9FYyUUsR5CbW",
        "outputId": "e1add648-1ad7-423f-8abf-8ed681989d59"
      },
      "source": [
        "features.head(40)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>agg_verbs_rate</th>\n",
              "      <th>agg_adjs_rate</th>\n",
              "      <th>agg_nouns_rate</th>\n",
              "      <th>agg_comm_probability</th>\n",
              "      <th>cars_cluster</th>\n",
              "      <th>is_comment</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>order_id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4a8adfc877d14e2d49d465c1bde8c839</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>b3457ff4fd872aa4afce47c482699d78</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>087ee63278c5468057f0da6f231527a7</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1110046556247e4f5e2ccb255f6fe294</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34b5c15740d45d8c3f4244ca7302fb33</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bf1923849c85cc892b2fc9abd91342e5</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>576c627ebcc4b09fcc34f4c4bf40dcd5</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>e75c9a3569dcbacfb5c112bc6d5be2b9</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.154038</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2b3ebd4d7a1b7bf92a3316216e2172a9</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>c66dbda2481f9c19a261936acc315cf2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>02595c7f5bed4b33594312d94a723bb8</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.153837</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7075b958f357a858d2e8d8aafc8203df</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.155363</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8fa813f58b9bc929ccdc8b1acf1d6405</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>e7ffe18935d59c9f5d2debf0a7db32fd</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4f12158c985a8fb59ec0db942d5a3d5c</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6cbfc1794fa5fc7beb699fc12c1b8cd1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4c5957e38526144e8a8bb7f7e786114f</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>f695817b827acccc5dffe179e981043b</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>b71ce617972d09f8bb05ec42645d66f1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.154240</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>dce305817063b02fe4fd0f4cacda5397</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>864bb8c9afbadf4252921aead40a7b4a</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>a8d0deba9907cf33c762434a86ab2225</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4fad7f0ba39d6f7367a0373dfdcf6886</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>e16f8ce8b0b7b96c70eaed45d9cbdeae</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11793311e39174e0c4fde5e6e73e7be3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45204d502a1abe8c86de14b4add833ca</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>c7dfc8e37be2bb2f241ff6b5a2734fad</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.155952</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17891d6507106e5b6322f9a64e65162e</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2be148c285bc2d8f1cdd5e054d3e6f85</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>a4d8daf8fee1d3f936b86e8386609fd2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fb2de622f289d05f1f1dca2a8655f4db</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19400470cc2356818c45b4c60641e899</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20033ae28309bab8d4becb7310887b94</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>c87695efe5f9696fb9e4c24a8ef089dd</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>443f2ecd471cf369823d4398f7fb26c2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>d1598b6d00f63b4470062724c50905b1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2e6912333d524867427cd5f94dad2969</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>08e8ac1b7ecb77063a478f9c7030bcd0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6f798479b40eebe2fa703d5cf31589b4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63ae02395d1d35c54886cd6bd1cef618</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                  agg_verbs_rate  ...  is_comment\n",
              "order_id                                          ...            \n",
              "4a8adfc877d14e2d49d465c1bde8c839             0.0  ...           0\n",
              "b3457ff4fd872aa4afce47c482699d78             0.0  ...           0\n",
              "087ee63278c5468057f0da6f231527a7             0.0  ...           0\n",
              "1110046556247e4f5e2ccb255f6fe294             0.0  ...           0\n",
              "34b5c15740d45d8c3f4244ca7302fb33             0.0  ...           0\n",
              "bf1923849c85cc892b2fc9abd91342e5             0.0  ...           0\n",
              "576c627ebcc4b09fcc34f4c4bf40dcd5             0.0  ...           0\n",
              "e75c9a3569dcbacfb5c112bc6d5be2b9             0.0  ...           1\n",
              "2b3ebd4d7a1b7bf92a3316216e2172a9             0.0  ...           0\n",
              "c66dbda2481f9c19a261936acc315cf2             0.0  ...           0\n",
              "02595c7f5bed4b33594312d94a723bb8             1.0  ...           1\n",
              "7075b958f357a858d2e8d8aafc8203df             1.0  ...           1\n",
              "8fa813f58b9bc929ccdc8b1acf1d6405             0.0  ...           0\n",
              "e7ffe18935d59c9f5d2debf0a7db32fd             0.0  ...           0\n",
              "4f12158c985a8fb59ec0db942d5a3d5c             0.0  ...           0\n",
              "6cbfc1794fa5fc7beb699fc12c1b8cd1             0.0  ...           0\n",
              "4c5957e38526144e8a8bb7f7e786114f             0.0  ...           0\n",
              "f695817b827acccc5dffe179e981043b             0.0  ...           0\n",
              "b71ce617972d09f8bb05ec42645d66f1             0.0  ...           1\n",
              "dce305817063b02fe4fd0f4cacda5397             0.0  ...           0\n",
              "864bb8c9afbadf4252921aead40a7b4a             0.0  ...           0\n",
              "a8d0deba9907cf33c762434a86ab2225             0.0  ...           0\n",
              "4fad7f0ba39d6f7367a0373dfdcf6886             0.0  ...           0\n",
              "e16f8ce8b0b7b96c70eaed45d9cbdeae             0.0  ...           0\n",
              "11793311e39174e0c4fde5e6e73e7be3             0.0  ...           0\n",
              "45204d502a1abe8c86de14b4add833ca             0.0  ...           0\n",
              "c7dfc8e37be2bb2f241ff6b5a2734fad             0.0  ...           1\n",
              "17891d6507106e5b6322f9a64e65162e             0.0  ...           0\n",
              "2be148c285bc2d8f1cdd5e054d3e6f85             0.0  ...           0\n",
              "a4d8daf8fee1d3f936b86e8386609fd2             0.0  ...           0\n",
              "fb2de622f289d05f1f1dca2a8655f4db             0.0  ...           0\n",
              "19400470cc2356818c45b4c60641e899             0.0  ...           0\n",
              "20033ae28309bab8d4becb7310887b94             0.0  ...           0\n",
              "c87695efe5f9696fb9e4c24a8ef089dd             0.0  ...           0\n",
              "443f2ecd471cf369823d4398f7fb26c2             0.0  ...           0\n",
              "d1598b6d00f63b4470062724c50905b1             0.0  ...           0\n",
              "2e6912333d524867427cd5f94dad2969             0.0  ...           0\n",
              "08e8ac1b7ecb77063a478f9c7030bcd0             0.0  ...           0\n",
              "6f798479b40eebe2fa703d5cf31589b4             0.0  ...           0\n",
              "63ae02395d1d35c54886cd6bd1cef618             0.0  ...           0\n",
              "\n",
              "[40 rows x 6 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKQmf6s2W-Do"
      },
      "source": [
        "### **Отсчет по фичам**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ao8TTcqexzNR"
      },
      "source": [
        "Скор без NLP фич - 0.5644\n",
        "Скор с NLP фичами:\n",
        "\n",
        "  - эмпирически: 0.6965 (если добавить все слова, а не по типу слов - результат хуже на 0.005) CV_mean: 0.7213110197676796, CV_std: 0.036754591689793115 - глаг+прил+сущ (но словарь сформирован для данных, на которых и идет предсказание, поэтому мб и такой скор)\n",
        "\n",
        "  - кластеризация и семантическая модель (doc2vec): 0.640-651 (фичи тип кластера, евклидово расстояние до каждого кластера) CV_mean roc_auc: 0.6625926120596789, CV_folds_score: [0.63023583 0.71938044 0.62881106 0.68139226 0.65314347] косинусное расстояние хоть обычно и юзают для текстов, но тут ухудшило результат(странно)\n",
        "  - эмпирические + кластеризация 0.668 CV_mean roc_auc: 0.6819522032387957, CV_folds_score: [0.62861001 0.74803329 0.66013863 0.70424985 0.66872923]\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRTmviXS5z2b"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}